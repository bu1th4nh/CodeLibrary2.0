# S3-compatible Storage with s3fs



## Setup

```python
import s3fs
storage_options = {
    'key': '',
    'secret': '',
    'endpoint_url': '',
}
s3 = s3fs.S3FileSystem(
    key=storage_options['key'],
    secret=storage_options['secret'],
    endpoint_url=storage_options['endpoint_url'],
    use_ssl=False,
)
```


## Read Dataframe using Pandas
```python   
import pandas as pd

df = pd.read_csv(
    's3://bucket-name/path/to/file.csv',
    storage_options=storage_options,
    sep = '\t',     # if use TSV, or other separator
    comment = '#',  # if there are comment lines in the file
    index_col = 0,  # if the first column is the index
    headers = None, # if there is no header row in the file
)
```

```python   
import pandas as pd

df = pd.read_parquet(
    's3://bucket-name/path/to/file.parquet',
    storage_options=storage_options,
)
```

You can write dataframes in a similar way using `to_csv` or `to_parquet` methods.


## Read/Write `*.h5mu` or `*.h5ad` or wwhatever files which doesn't support `storage_options`

### Read
```python
import tempfile
import mudata # or whatever library you use to read the file

with tempfile.NamedTemporaryFile(suffix='.h5mu') as temp_file:
    s3.get(f's3://path/to/file.h5mu', temp_file.name)
    data_simplified = mudata.read_h5mu(temp_file.name)
```

### Write
```python
import tempfile
import mudata # or whatever library you use to write the file

with tempfile.NamedTemporaryFile(suffix='.h5mu') as temp_file:
    data_everything.write(temp_file.name, compression='gzip', compression_opts=9)
    s3.put(temp_file.name, f's3://path/to/file.h5mu', recursive=True, storage_options=storage_options)
```



## File Operations

### List files in a bucket or folder
```python
s3.ls('s3://bucket-name/path/to/folder/')
```

### Move a folder
```python
s3.mv('s3://bucket-name/path/to/source-folder', 's3://bucket-name/path/to/destination-folder', recursive=True)
```